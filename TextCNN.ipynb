{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_src,x_col_name,y_col_name):\n",
    "    data = pd.read_csv(data_src,encoding='ISO-8859-1')\n",
    "    data.dropna(how='any',inplace=True)\n",
    "    input_X = data[x_col_name]\n",
    "    input_Y = data[y_col_name]\n",
    "    labels = list(set(input_Y))\n",
    "    one_hot_matrix = np.zeros([len(labels),len(labels)])\n",
    "    np.fill_diagonal(one_hot_matrix,1)\n",
    "    label_dict = dict(zip(labels,one_hot_matrix))\n",
    "    max_document_length = max(list(len(x.split(' ')) for x in input_X))\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    X = np.asarray(list(vocab_processor.fit_transform(input_X)))\n",
    "    Y = np.asarray(input_Y.apply(lambda y: label_dict[y]))\n",
    "    return X,Y,labels,vocab_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c9db05122529>:11: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "data_src = './data_repository/training_security_type.csv'\n",
    "X,Y,labels,vocab_processor = preprocess_data(data_src,'SECURITY_DESCRIPTION','SECURITY_TYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_epochs = 3\n",
    "sequence_length = X.shape[1]\n",
    "emb_dimension = 100\n",
    "n_classes = len(labels)\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "filter_sizes = [2,3,4]\n",
    "num_filters = 32\n",
    "dropout_keep_probs = 0.5\n",
    "l2_reg_lambda = 0\n",
    "batch_size = 32\n",
    "evaluate_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow Diagram of CNN\n",
    "input_x = tf.placeholder(dtype=tf.int32,shape=[None,sequence_length],name='input_x')\n",
    "input_y = tf.placeholder(dtype=tf.int32,shape=[None,n_classes],name='input_y')\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "\n",
    "# Embeddings Layer\n",
    "with tf.name_scope('embeddings'):\n",
    "    W = tf.Variable(tf.random_uniform([vocab_size,emb_dimension],-1.0,1.0),name='W')\n",
    "    embedded_chars = tf.nn.embedding_lookup(W,input_x)\n",
    "    embedded_chars_extended = tf.expand_dims(embedded_chars,-1)\n",
    "\n",
    "# Convolution Layer\n",
    "pooled_output = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope('conv-maxpool-%s' % i):\n",
    "        filter_shape = [filter_size,emb_dimension,1,num_filters]\n",
    "        W = tf.Variable(tf.truncated_normal(filter_shape,stddev=1.0), name='W')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[num_filters]), name='b')\n",
    "        conv = tf.nn.conv2d(\n",
    "            embedded_chars_extended,\n",
    "            W,\n",
    "            strides=[1,1,1,1],\n",
    "            padding='VALID',\n",
    "            name = 'conv_2d'\n",
    "        )\n",
    "\n",
    "        conv_relu = tf.nn.relu(tf.nn.bias_add(conv,b),name='relu')\n",
    "        conv_relu_maxpool = tf.nn.max_pool(\n",
    "            conv_relu,\n",
    "            ksize = [1,sequence_length-filter_size+1,1,1],\n",
    "            strides=[1,1,1,1],\n",
    "            padding='VALID',\n",
    "            name = 'pool'\n",
    "        )\n",
    "        pooled_output.append(conv_relu_maxpool)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected Layer\n",
    "number_of_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool = tf.concat(pooled_output,3)\n",
    "h_pool_flat = tf.reshape(h_pool,[-1,number_of_filters_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout\n",
    "with tf.name_scope('dropout'):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat,dropout_keep_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-47efac4bc61a>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final (unnormalized) scores and predictions\n",
    "# Keeping track of l2 regularization loss (optional)\n",
    "l2_loss = tf.constant(0.0)\n",
    "with tf.name_scope('output'):\n",
    "    W = tf.get_variable(\n",
    "        'W',\n",
    "        shape=[number_of_filters_total, n_classes],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[n_classes]), name='b')\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    scores = tf.nn.xw_plus_b(h_drop, W, b, name='scores')\n",
    "    probabilities = tf.nn.softmax(scores, name='probabilities')\n",
    "    predictions = tf.argmax(scores, 1, name='predictions')\n",
    "\n",
    "# Calculate mean cross-entropy loss\n",
    "with tf.name_scope('loss'):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(labels = input_y, logits = scores) #  only named arguments accepted            \n",
    "    loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
    "\n",
    "with tf.name_scope('num_correct'):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "    num_correct = tf.reduce_sum(tf.cast(correct_predictions, 'float'), name='num_correct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 2: split the original dataset into train and test sets\"\"\"\n",
    "x_, x_test, y_, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\"\"\"Step 3: shuffle the train set and split the train set into train and dev sets\"\"\"\n",
    "# shuffle_indices = np.random.permutation(np.arange(len(y_)))\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_)))\n",
    "x_shuffled = x_[shuffle_indices]\n",
    "y_shuffled = y_[shuffle_indices]\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_shuffled, y_shuffled, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step 5: build a graph and cnn object\"\"\"\n",
    "graph = tf.Graph()\n",
    "session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "sess = tf.Session(config=session_conf)\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "timestamp = str(int(time.time()))\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"trained_model_\" + timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One training step: train the model with one batch\n",
    "def train_step(x_batch, y_batch):\n",
    "    feed_dict = {\n",
    "        input_x: x_batch,\n",
    "        input_y: y_batch,\n",
    "        dropout_keep_prob: dropout_keep_probs}\n",
    "#     print(feed_dict)\n",
    "    _, step, losses, acc = sess.run([train_op, global_step, loss, accuracy], feed_dict)\n",
    "    print(step,' : ',losses,' : ',acc)\n",
    "\n",
    "# One evaluation step: evaluate the model with one batch\n",
    "def dev_step(x_batch, y_batch):\n",
    "    feed_dict = {input_x: x_batch, input_y: y_batch, dropout_keep_prob: 1.0}\n",
    "    step, losses, acc, num_corrects = sess.run([global_step, loss, accuracy, num_correct], feed_dict)\n",
    "    return num_corrects\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "\t\"\"\"Iterate the data batch by batch\"\"\"\n",
    "\tdata = np.array(data)\n",
    "\tdata_size = len(data)\n",
    "\tnum_batches_per_epoch = int(data_size / batch_size) + 1\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tif shuffle:\n",
    "\t\t\tshuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "\t\t\tshuffled_data = data[shuffle_indices]\n",
    "\t\telse:\n",
    "\t\t\tshuffled_data = data\n",
    "\n",
    "\t\tfor batch_num in range(num_batches_per_epoch):\n",
    "\t\t\tstart_index = batch_num * batch_size\n",
    "\t\t\tend_index = min((batch_num + 1) * batch_size, data_size)\n",
    "\t\t\tyield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word_to_id map since predict.py needs it\n",
    "vocab_processor.save(os.path.join(out_dir, \"vocab.pickle\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training starts here\n",
    "train_batches = batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "best_accuracy, best_at_step = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 0.6213333333333333 based on the best model /Users/sdas5/Data Science Study/Tensorflow practice/VocabProcessor/trained_model_1555009910/checkpoints/model-566\n",
      "The training is complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Step 6: train the cnn model with x_train and y_train (batch by batch)\"\"\"\n",
    "for train_batch in train_batches:\n",
    "#     print(' In training batch: ',len(train_batch))\n",
    "    x_train_batch, y_train_batch = zip(*train_batch)\n",
    "    train_step(x_train_batch, y_train_batch)\n",
    "    current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "    \"\"\"Step 6.1: evaluate the model with x_dev and y_dev (batch by batch)\"\"\"\n",
    "    if current_step % evaluate_every == 0:\n",
    "        dev_batches = batch_iter(list(zip(x_dev, y_dev)), batch_size, 1)\n",
    "        total_dev_correct = 0\n",
    "        for dev_batch in dev_batches:\n",
    "            x_dev_batch, y_dev_batch = zip(*dev_batch)\n",
    "            num_dev_correct = dev_step(x_dev_batch, y_dev_batch)\n",
    "            total_dev_correct += num_dev_correct\n",
    "\n",
    "        dev_accuracy = float(total_dev_correct) / len(y_dev)\n",
    "        print('Accuracy on dev set: {}'.format(dev_accuracy))\n",
    "\n",
    "        \"\"\"Step 6.2: save the model if it is the best based on accuracy on dev set\"\"\"\n",
    "        if dev_accuracy >= best_accuracy:\n",
    "            best_accuracy, best_at_step = dev_accuracy, current_step\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print('Saved model at {} at step {}'.format(path, best_at_step))\n",
    "            print('Best accuracy is {} at step {}'.format(best_accuracy, best_at_step))\n",
    "            \n",
    "            \n",
    "\"\"\"Step 7: predict x_test (batch by batch)\"\"\"\n",
    "test_batches = batch_iter(list(zip(x_test, y_test)), batch_size, 1)\n",
    "total_test_correct = 0\n",
    "for test_batch in test_batches:\n",
    "    x_test_batch, y_test_batch = zip(*test_batch)\n",
    "    num_test_correct = dev_step(x_test_batch, y_test_batch)\n",
    "    total_test_correct += num_test_correct\n",
    "\n",
    "test_accuracy = float(total_test_correct) / len(y_test)\n",
    "print('Accuracy on test set is {} based on the best model {}'.format(test_accuracy, path))\n",
    "print('The training is complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
